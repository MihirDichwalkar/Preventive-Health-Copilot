{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed690f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    temperature=0,\n",
    "    format=\"json\",\n",
    "    model=\"llama3.1:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbc2d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"I love programming\" in French is:\n",
      "\n",
      "\"J'adore le programmation.\"\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a887d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67cbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([add_numbers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9680f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful and may use tools.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb3dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OUTPUT:\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-11-13T18:08:10.708481Z', 'done': True, 'done_reason': 'stop', 'total_duration': 71713102700, 'load_duration': 63700720300, 'prompt_eval_count': 172, 'prompt_eval_duration': 1677572500, 'eval_count': 23, 'eval_duration': 6268309800, 'model_name': 'llama3.1:latest', 'model_provider': 'ollama'} id='lc_run--88f186b4-a42d-4f88-9ab7-758ac12560fc-0' tool_calls=[{'name': 'add_numbers', 'args': {'a': 12, 'b': 30}, 'id': 'b5d3baf4-f228-47f3-9aaa-17284f412046', 'type': 'tool_call'}] usage_metadata={'input_tokens': 172, 'output_tokens': 23, 'total_tokens': 195}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"What is 12 + 30?\"})\n",
    "print(\"MODEL OUTPUT:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9f8d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL RESULT: 42\n"
     ]
    }
   ],
   "source": [
    "if response.tool_calls:\n",
    "    call = response.tool_calls[0]\n",
    "    result = add_numbers.invoke(call[\"args\"])\n",
    "    print(\"TOOL RESULT:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c82662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-11-13T17:59:28.3236635Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8986444700, 'load_duration': 617100700, 'prompt_eval_count': 173, 'prompt_eval_duration': 1888862000, 'eval_count': 23, 'eval_duration': 6289215300, 'model_name': 'llama3.1:latest', 'model_provider': 'ollama'} id='lc_run--6870f271-83f7-4c6c-be03-d5540f1bf4ea-0' tool_calls=[{'name': 'add_numbers', 'args': {'a': 12, 'b': 30}, 'id': '17344f7e-1d09-4092-990b-b7dd49adf931', 'type': 'tool_call'}] usage_metadata={'input_tokens': 173, 'output_tokens': 23, 'total_tokens': 196}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "899090fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                1.0.5\n",
      "langchain-classic        1.0.0\n",
      "langchain-community      0.4.1\n",
      "langchain-core           1.0.4\n",
      "langchain-ollama         1.0.0\n",
      "langchain-openai         1.0.2\n",
      "langchain-text-splitters 1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | findstr langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8b42b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call invoked\n",
      "{\"output\": \"sunny\", \"temperature\": 25}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    temperature=0,\n",
    "    format=\"json\",\n",
    "    model=\"llama3.1:latest\"\n",
    ")\n",
    "\n",
    "# ... (Tools and LLM setup as before) ...\n",
    "# 1. Define a tool using the @tool decorator\n",
    "# The docstring becomes the tool's description, which Llama 3.1 uses for reasoning.\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str):\n",
    "    \"\"\"Fetches weather for given city\"\"\"\n",
    "    weather_data = { # Example data, use API in real-world\n",
    "        \"New York\": \"Sunny, 25°C\", \n",
    "        \"London\": \"Cloudy, 18°C\", \n",
    "        \"Tokyo\": \"Rainy, 22°C\"\n",
    "        }\n",
    "    return weather_data.get(city, \"Weather data not available.\")\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiplies two numbers and returns the result.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def power_of_two(x: float) -> float:\n",
    "    \"\"\"Calculates the square of a number.\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "# List of tools to bind to the model\n",
    "tools = [get_weather]\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. You the set of tools given to answer concisely.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 2) Format it for a particular query\n",
    "query = \"What is the weather in New York\"\n",
    "formatted_prompt = prompt_template.format_prompt(input=query)\n",
    "\n",
    "messages = formatted_prompt.to_messages()\n",
    "model = llm.bind_tools(tools)\n",
    "\n",
    "res = model.invoke(messages)\n",
    "messages.append(res)\n",
    "\n",
    "for tool_call in res.tool_calls:\n",
    "    print(\"Tool call invoked\")\n",
    "    selected_tool = {\"multiply\":multiply,\"power_of_two\":power_of_two,\"get_weather\":get_weather}[tool_call['name'].lower()]\n",
    "    tool_message = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_message)\n",
    "\n",
    "res = model.invoke(messages)\n",
    "messages.append(res)\n",
    "\n",
    "print(res.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c7101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.1\",\n",
    "    openai_api_key=\"EMPTY\",                      # Required param but unused\n",
    "    openai_api_base=\"http://localhost:11434/v1\", # Ollama endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3742a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihir\\AppData\\Local\\Temp\\ipykernel_10896\\1865365590.py:3: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee289ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "react_prompt = \"\"\"\n",
    "You are a Preventive Health Copilot.\n",
    "Use the ReAct pattern: Think → Reason → Act.\n",
    "\n",
    "User request: {query}\n",
    "\n",
    "Steps:\n",
    "1. Think through user goals.\n",
    "2. Decide if you need health tips or general advice.\n",
    "3. Answer clearly.\n",
    "\n",
    "Respond in this format:\n",
    "Thought:\n",
    "Action:\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# response = agent.invoke({\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(content=\"What is 10 × 7?\")\n",
    "#     ]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea09389f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying 10 by 7 is 70.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351248be",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.invoke(react_prompt.format(query=\"How do I reduce hypertension risk?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cacdb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the response using the ReAct pattern:\n",
      "\n",
      "**Think:**\n",
      "User goal is to reduce hypertension risk, which implies they are concerned about high blood pressure and want to take preventive measures.\n",
      "Current health tips available for reducing hypertension risk include lifestyle modifications (physical activity, diet, stress management), maintaining a healthy weight, not smoking, limiting alcohol consumption, managing salt intake.\n",
      "\n",
      "**Reason:**\n",
      "Deciding on the type of information to provide, I will offer specific actionable tips based on scientific evidence and consider the user's general understanding of health concepts. Since the goal is to reduce hypertension risk, providing general advice would be less effective than offering evidence-based tips tailored to this specific concern.\n",
      "\n",
      "**Act:**\n",
      "Based on steps 1 (think) and 2 (reason), here are my actionable recommendations:\n",
      "\n",
      "1. **Physical Activity**: Aim for at least 150 minutes of moderate-intensity aerobic physical activity or 75 minutes of vigorous-intensity aerobic physical activity per week, or an equivalent combination.\n",
      "2. **Eat Vegetables and Fruits Daily**: Focus on increasing consumption of leafy greens like kale and spinach, other vegetables, fruits, especially citrus ones like oranges for their folic acid content which is essential in lowering pressure inside blood vessels and also maintaining healthy cardiovascular health.\n",
      "3. **Reduce Sodium Intake**: Limit sodium intake to less than 2.3 grams (about one teaspoon) per day, focusing on consuming unseasoned foods where possible and choosing low-sodium options when needed.\n",
      "4. **Maintain Healthy Weight**: Work towards achieving and maintaining a healthy weight by monitoring your body mass index (BMI), aiming for it to be between 18.5 and 24.9 kg/m².\n",
      "\n",
      "These tips aim to directly address the user's concern of reducing hypertension risk through evidence-backed strategies that are easy to adopt into their daily routine.\n"
     ]
    }
   ],
   "source": [
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e1d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "healthcopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
